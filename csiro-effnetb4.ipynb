{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7539b16f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T03:49:44.161517Z",
     "iopub.status.busy": "2025-11-21T03:49:44.161246Z",
     "iopub.status.idle": "2025-11-21T03:49:55.956687Z",
     "shell.execute_reply": "2025-11-21T03:49:55.955832Z"
    },
    "papermill": {
     "duration": 11.800335,
     "end_time": "2025-11-21T03:49:55.958050",
     "exception": false,
     "start_time": "2025-11-21T03:49:44.157715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Siap menggunakan EfficientNet-B4 pada: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# --- KONFIGURASI MODEL B4 ---\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_fold': 5,            \n",
    "    'epochs': 15,           \n",
    "    'img_size': 380,        # UPDATE: B4 butuh resolusi 380x380\n",
    "    'batch_size': 8,        # UPDATE: Turunkan batch size biar GPU gak meledak (B4 makan memori)\n",
    "    'learning_rate': 1e-4,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \n",
    "    # PENTING: Ganti path ini sesuai lokasi file .pth B4 yang Anda Add Data\n",
    "    # Contoh path umum di kaggle (silakan cek input folder Anda):\n",
    "    'weights_path': '/kaggle/input/efficientnetb4weight/pytorch/default/1/efficientnet_b4_rwightman-23ab8bcd.pth' \n",
    "    # Jika tidak ketemu file-nya, kosongkan ('') tapi nyalakan internet saat training pertama\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CONFIG['seed'])\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f\"‚úÖ Siap menggunakan EfficientNet-B4 pada: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d415476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T03:49:55.963184Z",
     "iopub.status.busy": "2025-11-21T03:49:55.962665Z",
     "iopub.status.idle": "2025-11-21T03:49:56.035210Z",
     "shell.execute_reply": "2025-11-21T03:49:56.034362Z"
    },
    "papermill": {
     "duration": 0.07621,
     "end_time": "2025-11-21T03:49:56.036379",
     "exception": false,
     "start_time": "2025-11-21T03:49:55.960169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Train Image Dir: /kaggle/input/csiro-biomass/train\n"
     ]
    }
   ],
   "source": [
    "INPUT_ROOT = '/kaggle/input'\n",
    "DATASET_DIR = ''\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_ROOT):\n",
    "    if 'train.csv' in filenames:\n",
    "        DATASET_DIR = dirname\n",
    "        break\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATASET_DIR, '/kaggle/input/csiro-biomass/train.csv')\n",
    "TEST_CSV = os.path.join(DATASET_DIR, '/kaggle/input/csiro-biomass/test.csv')\n",
    "\n",
    "# Cari folder gambar\n",
    "possible_dirs = [os.path.join(DATASET_DIR, 'train'), os.path.join(DATASET_DIR, 'images'), DATASET_DIR]\n",
    "TRAIN_IMG_DIR = None\n",
    "for d in possible_dirs:\n",
    "    if os.path.exists(d) and any(f.endswith('.jpg') for f in os.listdir(d)[:5]):\n",
    "        TRAIN_IMG_DIR = d\n",
    "        break\n",
    "TEST_IMG_DIR = TRAIN_IMG_DIR.replace('train', 'test') if TRAIN_IMG_DIR else None\n",
    "\n",
    "# Load Data & Log Transform\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_pivot = train_df.pivot_table(index='image_path', columns='target_name', values='target').reset_index()\n",
    "\n",
    "target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "for col in target_cols:\n",
    "    train_pivot[col] = np.log1p(train_pivot[col]) # Log Transform\n",
    "\n",
    "print(f\"üìÇ Train Image Dir: {TRAIN_IMG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9fea09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T03:49:56.041478Z",
     "iopub.status.busy": "2025-11-21T03:49:56.040907Z",
     "iopub.status.idle": "2025-11-21T03:49:56.048750Z",
     "shell.execute_reply": "2025-11-21T03:49:56.048050Z"
    },
    "papermill": {
     "duration": 0.011411,
     "end_time": "2025-11-21T03:49:56.049774",
     "exception": false,
     "start_time": "2025-11-21T03:49:56.038363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transforms=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        self.target_cols = target_cols\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = os.path.basename(row['image_path'])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new('RGB', (CONFIG['img_size'], CONFIG['img_size']))\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        if self.is_test:\n",
    "            return image, row['image_path']\n",
    "        else:\n",
    "            targets = row[self.target_cols].values.astype(np.float32)\n",
    "            return image, torch.tensor(targets)\n",
    "\n",
    "# Transforms untuk EfficientNet-B4 (380x380)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(30), # Rotasi lebih berani\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Augmentasi warna\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac258d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T03:49:56.054502Z",
     "iopub.status.busy": "2025-11-21T03:49:56.053903Z",
     "iopub.status.idle": "2025-11-21T03:49:56.060034Z",
     "shell.execute_reply": "2025-11-21T03:49:56.059333Z"
    },
    "papermill": {
     "duration": 0.009646,
     "end_time": "2025-11-21T03:49:56.061150",
     "exception": false,
     "start_time": "2025-11-21T03:49:56.051504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobustBiomassModel(nn.Module):\n",
    "    def __init__(self, weights_path=None):\n",
    "        super(RobustBiomassModel, self).__init__()\n",
    "        \n",
    "        # 1. Load B4 (Tanpa Internet)\n",
    "        self.backbone = models.efficientnet_b4(weights=None)\n",
    "        \n",
    "        # 2. Load Weights Manual\n",
    "        if weights_path and os.path.exists(weights_path):\n",
    "            try:\n",
    "                state_dict = torch.load(weights_path)\n",
    "                # Kadang key di file .pth beda format, ini handle basicnya\n",
    "                self.backbone.load_state_dict(state_dict, strict=False) \n",
    "                print(\"‚úÖ Pretrained B4 weights berhasil dimuat!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Warning: {e}\")\n",
    "        \n",
    "        # 3. Modifikasi Head (Lebih Besar dari B3)\n",
    "        num_features = self.backbone.classifier[1].in_features # B4 fiturnya 1792\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024), # Layer lebih lebar\n",
    "            nn.SiLU(), \n",
    "            nn.Dropout(0.4), # Dropout lebih tinggi (0.4) untuk cegah overfitting\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7786ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T03:49:56.065707Z",
     "iopub.status.busy": "2025-11-21T03:49:56.065497Z",
     "iopub.status.idle": "2025-11-21T04:07:23.862054Z",
     "shell.execute_reply": "2025-11-21T04:07:23.861057Z"
    },
    "papermill": {
     "duration": 1047.801946,
     "end_time": "2025-11-21T04:07:23.864904",
     "exception": false,
     "start_time": "2025-11-21T03:49:56.062958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FOLD 1/5 (B4 Model) ==========\n",
      "‚úÖ Pretrained B4 weights berhasil dimuat!\n",
      "  üèÜ Best Val Loss Fold 1: 0.3108\n",
      "\n",
      "========== FOLD 2/5 (B4 Model) ==========\n",
      "‚úÖ Pretrained B4 weights berhasil dimuat!\n",
      "  üèÜ Best Val Loss Fold 2: 0.3447\n",
      "\n",
      "========== FOLD 3/5 (B4 Model) ==========\n",
      "‚úÖ Pretrained B4 weights berhasil dimuat!\n",
      "  üèÜ Best Val Loss Fold 3: 0.2887\n",
      "\n",
      "========== FOLD 4/5 (B4 Model) ==========\n",
      "‚úÖ Pretrained B4 weights berhasil dimuat!\n",
      "  üèÜ Best Val Loss Fold 4: 0.2351\n",
      "\n",
      "========== FOLD 5/5 (B4 Model) ==========\n",
      "‚úÖ Pretrained B4 weights berhasil dimuat!\n",
      "  üèÜ Best Val Loss Fold 5: 0.3032\n",
      "\n",
      "‚ú® Average CV Score: 0.2965\n"
     ]
    }
   ],
   "source": [
    "def train_fold(fold, train_idx, val_idx):\n",
    "    print(f\"\\n{'='*10} FOLD {fold+1}/{CONFIG['n_fold']} (B4 Model) {'='*10}\")\n",
    "    \n",
    "    train_sub = train_pivot.iloc[train_idx].reset_index(drop=True)\n",
    "    val_sub = train_pivot.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        BiomassDataset(train_sub, TRAIN_IMG_DIR, transforms=data_transforms['train']),\n",
    "        batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        BiomassDataset(val_sub, TRAIN_IMG_DIR, transforms=data_transforms['val']),\n",
    "        batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2\n",
    "    )\n",
    "    \n",
    "    model = RobustBiomassModel(weights_path=CONFIG['weights_path']).to(CONFIG['device'])\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=1.0) \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=1e-4)\n",
    "    \n",
    "    # Scheduler diperlambat agar belajarnya lebih detail\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['epochs'], eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for imgs, targets in train_loader:\n",
    "            imgs, targets = imgs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            optimizer.zero_grad()\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs, targets = imgs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "                out = model(imgs)\n",
    "                loss = criterion(out, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f'model_fold_{fold}.pth')\n",
    "            \n",
    "    print(f\"  üèÜ Best Val Loss Fold {fold+1}: {best_loss:.4f}\")\n",
    "    return best_loss\n",
    "\n",
    "# Jalankan K-Fold\n",
    "kf = KFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n",
    "losses = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_pivot)):\n",
    "    loss = train_fold(fold, train_idx, val_idx)\n",
    "    losses.append(loss)\n",
    "print(f\"\\n‚ú® Average CV Score: {np.mean(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14b5db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T04:07:23.870413Z",
     "iopub.status.busy": "2025-11-21T04:07:23.870162Z",
     "iopub.status.idle": "2025-11-21T04:07:27.009056Z",
     "shell.execute_reply": "2025-11-21T04:07:27.008194Z"
    },
    "papermill": {
     "duration": 3.143279,
     "end_time": "2025-11-21T04:07:27.010428",
     "exception": false,
     "start_time": "2025-11-21T04:07:23.867149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Memulai Inference B4 (5 Model Ensemble)...\n",
      "üéâ Submission Created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>0.273096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>14.639925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>21.390858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>38.992695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>21.511496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id     target\n",
       "0  ID1001187975__Dry_Clover_g   0.273096\n",
       "1    ID1001187975__Dry_Dead_g  14.639925\n",
       "2   ID1001187975__Dry_Green_g  21.390858\n",
       "3   ID1001187975__Dry_Total_g  38.992695\n",
       "4         ID1001187975__GDM_g  21.511496"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_ensemble(test_csv_path, img_dir):\n",
    "    print(\"\\nüöÄ Memulai Inference B4 (5 Model Ensemble)...\")\n",
    "    \n",
    "    test_df_long = pd.read_csv(test_csv_path)\n",
    "    unique_imgs = test_df_long[['image_path']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    test_ds = BiomassDataset(unique_imgs, img_dir, data_transforms['val'], is_test=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    models_list = []\n",
    "    for i in range(CONFIG['n_fold']):\n",
    "        model = RobustBiomassModel(weights_path=None) \n",
    "        model.load_state_dict(torch.load(f'model_fold_{i}.pth'))\n",
    "        model.to(CONFIG['device'])\n",
    "        model.eval()\n",
    "        models_list.append(model)\n",
    "    \n",
    "    image_path_keys = []\n",
    "    final_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, paths in test_dl:\n",
    "            xb = xb.to(CONFIG['device'])\n",
    "            batch_preds = []\n",
    "            for model in models_list:\n",
    "                out = model(xb)\n",
    "                out = torch.expm1(out)  # Reverse Log\n",
    "                out = torch.relu(out)\n",
    "                batch_preds.append(out.cpu().numpy())\n",
    "            \n",
    "            avg_preds = np.mean(batch_preds, axis=0)\n",
    "            \n",
    "            for path, pred in zip(paths, avg_preds):\n",
    "                image_path_keys.append(os.path.basename(path))\n",
    "                final_preds.append(pred)\n",
    "                \n",
    "    preds_wide = pd.DataFrame(final_preds, columns=target_cols)\n",
    "    preds_wide['image_path_key'] = image_path_keys\n",
    "    \n",
    "    preds_long = preds_wide.melt(id_vars=['image_path_key'], value_vars=target_cols, var_name='target_name', value_name='target')\n",
    "    test_df_long['image_path_key'] = test_df_long['image_path'].apply(os.path.basename)\n",
    "    \n",
    "    submission = pd.merge(test_df_long[['sample_id', 'image_path_key', 'target_name']], preds_long, on=['image_path_key', 'target_name'], how='left')\n",
    "    submission = submission[['sample_id', 'target']]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"üéâ Submission Created!\")\n",
    "    return submission.head()\n",
    "\n",
    "predict_ensemble(TEST_CSV, TEST_IMG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "modelId": 510040,
     "modelInstanceId": 494650,
     "sourceId": 654596,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 510558,
     "modelInstanceId": 495148,
     "sourceId": 655185,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1068.497357,
   "end_time": "2025-11-21T04:07:28.934586",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T03:49:40.437229",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
